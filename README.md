<div align="center"> <img src="logo.png" alt="logo" height="150"> <h1 style="font-size: 16px; font-weight: bold;"> CX-Mind: A Pioneering Multimodal Large Language Model for Interleaved Reasoning in Chest X-ray via Curriculum-Guided Reinforcement Learning </h1> <br> <a href="[https://arxiv.org/abs/2505.14362](https://arxiv.org/abs/2508.03733)"> <img src="https://img.shields.io/badge/ArXiv-CXMind-brown?logo=arxiv" alt="Paper"> </a> <a href="https://huggingface.co/SII-JasperLi77/CX-Mind"> <img src="https://img.shields.io/badge/ðŸ¤— huggingface-Model-purple" alt="checkpoint"> </a> </div>



## CX-Mind

<figure style="margin:16px auto; text-align:center;">
  <img src="overview.png"
       style="max-width:100%; width:900px; height:auto; border-radius:12px; box-shadow:0 4px 24px rgba(0,0,0,.08);" />
  <figcaption style="font-size:14px; color:#666; margin-top:8px;">
  </figcaption>
</figure>

Key insights:
- Large chest X-ray dataset with over 2 million entries across 23 datasets.
- Novel training strategy boosts medical knowledge and reasoning in models.
- First interleaved reasoning approach for clear medical model interpretability.
- CX-Mind surpasses top medical reasoning models in extensive benchmark tests.
- Real-world clinical dataset validates CX-Mindâ€™s utility with expert reviews.



##  Quick Start


### Environment Setup
```bash
# Follow the Easy-R1 official installation procedure
git clone https://github.com/SII-zyj/CX-Mind-private.git
cd CX-Mind
pip install -e .
```

### Start Training



## Programming Guide


### General Introduction


### Training Config



### Use your own data


### Implement your own tools


**Important**:



## Star Chart
<h2>Star Chart</h2>
<p align="center">
  <a href="https://star-history.com/#SII-zyj/CX-Mind-private&Date">
    <img src="https://api.star-history.com/svg?repos=SII-zyj/CX-Mind-private&type=Date" alt="Star History Chart" width="800">
  </a>
</p>

## Licence

This project is released under [Apache licence](./LICENSE).

## Citation

```
@article{li2025cx,
  title={CX-Mind: A Pioneering Multimodal Large Language Model for Interleaved Reasoning in Chest X-ray via Curriculum-Guided Reinforcement Learning},
  author={Li, Wenjie and Zhang, Yujie and Sun, Haoran and Li, Yueqi and Zhang, Fanrui and Xu, Mengzhe and Clausich, Victoria Borja and Mellin, Sade and Yang, Renhao and Wang, Chenrun and others},
  journal={arXiv preprint arXiv:2508.03733},
  year={2025}
}
```
